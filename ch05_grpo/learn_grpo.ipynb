{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapters 4 & 5: Policy Gradients and GRPO\n",
    "\n",
    "**Goal:** Understand how GRPO (Group Relative Policy Optimization) uses reward signals to update model parameters, and why it eliminates the need for a critic network.\n",
    "\n",
    "This notebook covers both Chapter 4 (REINFORCE theory) and Chapter 5 (GRPO implementation), since `grpo.py` implements the ideas from both chapters.\n",
    "\n",
    "---\n",
    "\n",
    "## The Central Problem\n",
    "\n",
    "We have:\n",
    "- A model that generates text (Chapter 2)\n",
    "- A reward function that scores responses (Chapter 3)\n",
    "\n",
    "**Question:** How do we use the scalar reward to compute gradients and update the model weights?\n",
    "\n",
    "**The difficulty:** Sampling is **non-differentiable**. When we call `model.generate()`, the function `torch.multinomial()` selects discrete tokens. We can't backpropagate through a discrete selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Log-Derivative Trick (REINFORCE)\n",
    "\n",
    "### Chapter 4's Key Insight\n",
    "\n",
    "We can't differentiate through sampling, but we CAN differentiate through **log-probabilities**.\n",
    "\n",
    "The REINFORCE algorithm:\n",
    "1. **Sample** a response (no gradients needed)\n",
    "2. **Compute the log-probability** of that response under the current model (this IS differentiable)\n",
    "3. **Multiply** by the reward\n",
    "4. **Backpropagate**\n",
    "\n",
    "### The Math\n",
    "\n",
    "Our objective is to maximize expected reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta}[R(s, a)]$$\n",
    "\n",
    "Using the log-derivative trick:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta}[R(s, a) \\cdot \\nabla_\\theta \\log \\pi_\\theta(a|s)]$$\n",
    "\n",
    "In practice, we approximate this expectation with a single sample:\n",
    "\n",
    "$$\\nabla_\\theta J \\approx R(s, a) \\cdot \\nabla_\\theta \\log \\pi_\\theta(a|s)$$\n",
    "\n",
    "And since PyTorch minimizes loss:\n",
    "\n",
    "$$\\text{loss} = -R(s, a) \\cdot \\log \\pi_\\theta(a|s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuitive demonstration of why this works\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Imagine a simple model with 3 possible tokens\n",
    "logits = torch.tensor([1.0, 0.5, -0.5], requires_grad=True)\n",
    "probs = F.softmax(logits, dim=0)\n",
    "\n",
    "print(\"Initial token probabilities:\")\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"  Token {i}: {p.item():.4f}\")\n",
    "\n",
    "# Suppose we sampled Token 0 and got reward +1.0\n",
    "sampled_token = 0\n",
    "reward = 1.0\n",
    "\n",
    "# REINFORCE loss\n",
    "log_prob = F.log_softmax(logits, dim=0)[sampled_token]\n",
    "loss = -reward * log_prob\n",
    "\n",
    "print(f\"\\nSampled token: {sampled_token}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Log prob of sampled token: {log_prob.item():.4f}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients on logits: {logits.grad.tolist()}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Negative gradient on Token 0 --> optimizer will INCREASE its logit\")\n",
    "print(\"  Positive gradients on Tokens 1,2 --> optimizer will DECREASE their logits\")\n",
    "print(\"  Net effect: Token 0 becomes MORE likely (reward was positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with a NEGATIVE reward\n",
    "logits2 = torch.tensor([1.0, 0.5, -0.5], requires_grad=True)\n",
    "\n",
    "# Suppose we sampled Token 0 but got reward -1.0\n",
    "sampled_token = 0\n",
    "reward = -1.0\n",
    "\n",
    "log_prob = F.log_softmax(logits2, dim=0)[sampled_token]\n",
    "loss = -reward * log_prob\n",
    "loss.backward()\n",
    "\n",
    "print(\"With negative reward:\")\n",
    "print(f\"  Gradients on logits: {logits2.grad.tolist()}\")\n",
    "print(\"  Positive gradient on Token 0 --> optimizer will DECREASE its logit\")\n",
    "print(\"  Net effect: Token 0 becomes LESS likely (reward was negative)\")\n",
    "print()\n",
    "print(\"This is the core of RL: positive reward = do more of this,\")\n",
    "print(\"negative reward = do less of this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Variance Problem & Baselines\n",
    "\n",
    "### Why raw REINFORCE has high variance\n",
    "\n",
    "If all rewards are positive (say, ranging from 0.1 to 1.0), then EVERY response gets \"encouraged\" - even the worst ones. The gradient signal is noisy.\n",
    "\n",
    "### The Solution: Subtract a Baseline\n",
    "\n",
    "Instead of using raw rewards, use **advantages**:\n",
    "\n",
    "$$A(s, a_i) = R(s, a_i) - b(s)$$\n",
    "\n",
    "Where $b(s)$ is a baseline. Responses better than the baseline get positive advantages (encouraged), those worse get negative advantages (discouraged).\n",
    "\n",
    "**Mathematical guarantee:** Subtracting a constant baseline does NOT change the expected gradient direction - it only reduces variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the baseline's effect\n",
    "\n",
    "# Scenario: 4 responses with all-positive rewards\n",
    "rewards_raw = [0.8, 0.3, 0.9, 0.2]\n",
    "\n",
    "# Without baseline: ALL get encouraged (all positive)\n",
    "print(\"WITHOUT baseline:\")\n",
    "for i, r in enumerate(rewards_raw):\n",
    "    direction = \"encourage\" if r > 0 else \"discourage\"\n",
    "    print(f\"  Response {i}: reward={r:+.1f} --> {direction}\")\n",
    "print(\"  Problem: Even the worst response (0.2) gets encouraged!\")\n",
    "\n",
    "print()\n",
    "\n",
    "# With baseline: only above-average get encouraged\n",
    "baseline = sum(rewards_raw) / len(rewards_raw)\n",
    "advantages = [r - baseline for r in rewards_raw]\n",
    "\n",
    "print(f\"WITH baseline (mean = {baseline:.2f}):\")\n",
    "for i, (r, adv) in enumerate(zip(rewards_raw, advantages)):\n",
    "    direction = \"ENCOURAGE\" if adv > 0 else \"DISCOURAGE\"\n",
    "    print(f\"  Response {i}: reward={r:+.1f}, advantage={adv:+.2f} --> {direction}\")\n",
    "print(\"  Now only above-average responses get encouraged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: From REINFORCE to GRPO\n",
    "\n",
    "### The Critic Problem\n",
    "\n",
    "**PPO** (used for ChatGPT) maintains a **critic network** $V(s)$ that estimates the expected reward for each state. This serves as the baseline. But:\n",
    "\n",
    "- The critic needs to be as large as the policy (~8B parameters)\n",
    "- It needs its own optimizer and learning rate schedule  \n",
    "- Total memory: ~48 GB (impossible on a 24 GB GPU)\n",
    "\n",
    "### The GRPO Insight\n",
    "\n",
    "**Instead of *predicting* the expected reward with a critic, *calculate* it directly!**\n",
    "\n",
    "For each prompt:\n",
    "1. Generate a **group** of G responses\n",
    "2. Score all G responses with the reward function\n",
    "3. Use the **group mean** as the baseline\n",
    "\n",
    "This is a Monte Carlo estimate of the expected reward - no neural network needed!\n",
    "\n",
    "### The GRPO Loss\n",
    "\n",
    "$$\\mathcal{L}_{GRPO}(\\theta) = -\\frac{1}{G} \\sum_{i=1}^{G} \\hat{A}(s, a_i) \\cdot \\log \\pi_\\theta(a_i | s)$$\n",
    "\n",
    "Where:\n",
    "$$\\hat{A}(s, a_i) = R(s, a_i) - \\frac{1}{G} \\sum_{j=1}^{G} R(s, a_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: `compute_log_probs()` - Line by Line\n",
    "\n",
    "This function computes $\\log \\pi_\\theta(a|s)$ - the log-probability of a response under the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_log_probs(model, tokenizer, prompt: str, response: str) -> torch.Tensor:\n",
    "    \"\"\"Compute the total log-probability of a response given a prompt.\n",
    "    \n",
    "    This is the DIFFERENTIABLE computation that allows gradient flow.\n",
    "    Even though we generated the response with torch.no_grad(),\n",
    "    we can compute its probability WITH gradients.\n",
    "    \"\"\"\n",
    "    # STEP 1: Concatenate prompt + response into one sequence\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # STEP 2: Find where the prompt ends and response begins\n",
    "    prompt_len = len(tokenizer(prompt, return_tensors=\"pt\").input_ids[0])\n",
    "    \n",
    "    # STEP 3: Forward pass through the model (WITH gradient tracking!)\n",
    "    # Note: NO torch.no_grad() here - we need gradients for learning\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # STEP 4: Extract logits for the response tokens only\n",
    "    # outputs.logits shape: [1, seq_len, vocab_size]\n",
    "    # We want logits that PREDICT response tokens:\n",
    "    #   logits[prompt_len-1] predicts response[0]\n",
    "    #   logits[prompt_len]   predicts response[1]\n",
    "    #   ...\n",
    "    #   logits[-2]           predicts response[-1]\n",
    "    logits = outputs.logits[0, prompt_len-1:-1]\n",
    "    \n",
    "    # STEP 5: Get the actual response token IDs\n",
    "    target_ids = inputs.input_ids[0, prompt_len:]\n",
    "    \n",
    "    # STEP 6: Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # STEP 7: Gather the log-prob of each actual response token\n",
    "    # This picks out the probability the model assigned to the tokens\n",
    "    # that were actually generated\n",
    "    token_log_probs = log_probs.gather(1, target_ids.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # STEP 8: Sum to get total sequence log-probability\n",
    "    # log P(response|prompt) = sum of log P(token_i | prompt, token_1..i-1)\n",
    "    return token_log_probs.sum()\n",
    "\n",
    "\n",
    "print(\"compute_log_probs() defined.\")\n",
    "print(\"This function computes log P(response | prompt) under the current model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the logit indexing\n",
    "\n",
    "This is the trickiest part. Let's visualize:\n",
    "\n",
    "```\n",
    "Full sequence:  [P0, P1, P2, R0, R1, R2]\n",
    "                 |---prompt--|  |--response-|\n",
    "                 prompt_len=3\n",
    "\n",
    "Logits:         [L0, L1, L2, L3, L4, L5]\n",
    "                  |    |    |    |    |    |\n",
    "                  v    v    v    v    v    v\n",
    "Predicts:       [P1,  P2,  R0,  R1,  R2,  <next>]\n",
    "\n",
    "We want logits that predict R0, R1, R2:\n",
    "  L2 predicts R0  (index: prompt_len-1 = 2)\n",
    "  L3 predicts R1\n",
    "  L4 predicts R2  (index: -2, which is len-2)\n",
    "\n",
    "So: logits[prompt_len-1 : -1] = logits[2:5] = [L2, L3, L4]\n",
    "And: target_ids = input_ids[prompt_len:] = [R0, R1, R2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual demonstration of the indexing logic\n",
    "\n",
    "# Simulating a sequence of 6 tokens: 3 prompt + 3 response\n",
    "prompt_tokens = [\"What\", \"is\", \"2+2\"]\n",
    "response_tokens = [\"The\", \"answer\", \"4\"]\n",
    "full_sequence = prompt_tokens + response_tokens\n",
    "\n",
    "prompt_len = len(prompt_tokens)  # 3\n",
    "\n",
    "print(\"Full sequence:\")\n",
    "for i, token in enumerate(full_sequence):\n",
    "    label = \"prompt\" if i < prompt_len else \"response\"\n",
    "    print(f\"  Position {i}: '{token}' ({label})\")\n",
    "\n",
    "print(f\"\\nprompt_len = {prompt_len}\")\n",
    "print()\n",
    "\n",
    "print(\"Logit-to-prediction mapping:\")\n",
    "for i in range(len(full_sequence) - 1):\n",
    "    predicted = full_sequence[i + 1]\n",
    "    in_range = prompt_len - 1 <= i < len(full_sequence) - 1\n",
    "    marker = \" <-- USED\" if in_range else \"\"\n",
    "    print(f\"  Logit[{i}] (at '{full_sequence[i]}') predicts '{predicted}'{marker}\")\n",
    "\n",
    "print(f\"\\nSlice: logits[{prompt_len-1}:-1] = logits[{prompt_len-1}:{len(full_sequence)-1}]\")\n",
    "print(f\"Targets: input_ids[{prompt_len}:] = {response_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `gather()`\n",
    "\n",
    "The `gather` operation picks specific values from a tensor along a dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating gather\n",
    "\n",
    "# Imagine log_probs for 3 tokens, vocabulary size 5\n",
    "vocab = [\"the\", \"answer\", \"is\", \"4\", \"hello\"]\n",
    "log_probs = torch.tensor([\n",
    "    [-2.0, -0.5, -3.0, -4.0, -5.0],  # Position 0: \"answer\" most likely\n",
    "    [-3.0, -4.0, -0.3, -2.0, -5.0],  # Position 1: \"is\" most likely\n",
    "    [-4.0, -5.0, -3.0, -0.1, -6.0],  # Position 2: \"4\" most likely\n",
    "])\n",
    "\n",
    "# The actual response tokens (as indices into vocab)\n",
    "target_ids = torch.tensor([1, 2, 3])  # \"answer\", \"is\", \"4\"\n",
    "\n",
    "# gather picks out the log-prob of the actual token at each position\n",
    "token_log_probs = log_probs.gather(1, target_ids.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "print(\"Log-probability table (rows=position, cols=vocab):\")\n",
    "print(f\"{'':>12}\", end=\"\")\n",
    "for v in vocab:\n",
    "    print(f\"{v:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Position {i}:\", end=\"\")\n",
    "    for j in range(5):\n",
    "        marker = \" *\" if j == target_ids[i] else \"  \"\n",
    "        print(f\"{log_probs[i][j].item():>6.1f}{marker}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nGathered log-probs (* marks): {token_log_probs.tolist()}\")\n",
    "print(f\"Sum (total sequence log-prob): {token_log_probs.sum().item():.1f}\")\n",
    "print(f\"\\nThis means the model assigned probability exp({token_log_probs.sum().item():.1f})\")\n",
    "print(f\"= {torch.exp(token_log_probs.sum()).item():.6f} to the response 'answer is 4'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: `grpo_step()` - The Complete Algorithm\n",
    "\n",
    "This is the heart of the system. Let's walk through it piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "def grpo_step(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    ground_truth: str,\n",
    "    reward_fn: Callable[[str, str], float],\n",
    "    G: int = 4,                 # Group size\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 256,\n",
    ") -> tuple[float, float, list[str]]:\n",
    "    \"\"\"\n",
    "    One step of GRPO:\n",
    "    1. Generate G responses for a single prompt\n",
    "    2. Score all G responses\n",
    "    3. Compute advantages using group mean as baseline\n",
    "    4. Compute policy gradient loss\n",
    "    5. Backpropagate\n",
    "    \n",
    "    Returns: (loss_value, mean_reward, responses)\n",
    "    \"\"\"\n",
    "    model.train()  # Enable dropout etc.\n",
    "    \n",
    "    # ============================================================\n",
    "    # PHASE 1: Generate G responses (INFERENCE - no gradients)\n",
    "    # ============================================================\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    responses = []\n",
    "    for _ in range(G):\n",
    "        with torch.no_grad():  # No gradients during generation!\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        response = tokenizer.decode(\n",
    "            output_ids[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        responses.append(response)\n",
    "    \n",
    "    # ============================================================\n",
    "    # PHASE 2: Score all responses and compute advantages\n",
    "    # ============================================================\n",
    "    rewards = torch.tensor(\n",
    "        [reward_fn(r, ground_truth) for r in responses],\n",
    "        dtype=torch.float32,\n",
    "        device=model.device,\n",
    "    )\n",
    "    \n",
    "    # The GRPO baseline: group mean\n",
    "    baseline = rewards.mean()\n",
    "    advantages = rewards - baseline\n",
    "    \n",
    "    # ============================================================\n",
    "    # PHASE 3: Compute GRPO loss (WITH gradients)\n",
    "    # ============================================================\n",
    "    loss = torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "    \n",
    "    for response, adv in zip(responses, advantages):\n",
    "        # Skip if advantage is ~0 (all responses got same reward)\n",
    "        # Nothing to learn when there's no contrast\n",
    "        if adv.abs() < 1e-8:\n",
    "            continue\n",
    "        \n",
    "        # Compute log P(response | prompt) - THIS IS DIFFERENTIABLE\n",
    "        log_prob = compute_log_probs(model, tokenizer, formatted_prompt, response)\n",
    "        \n",
    "        # Accumulate: loss = -(1/G) * sum(advantage * log_prob)\n",
    "        loss = loss - adv * log_prob / G\n",
    "    \n",
    "    # ============================================================\n",
    "    # PHASE 4: Backpropagate\n",
    "    # ============================================================\n",
    "    if loss.requires_grad:\n",
    "        loss.backward()  # Computes gradients on all LoRA parameters\n",
    "    \n",
    "    return loss.item(), rewards.mean().item(), responses\n",
    "\n",
    "\n",
    "print(\"grpo_step() defined.\")\n",
    "print(\"This function performs one complete GRPO update step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Phases Visualized\n",
    "\n",
    "```\n",
    "PHASE 1: GENERATION (torch.no_grad)\n",
    "  Prompt: \"What is 15 * 23?\"\n",
    "  |\n",
    "  +--> Response 1: \"The answer is 345\"     (correct)\n",
    "  +--> Response 2: \"I think it's 355\"      (wrong)  \n",
    "  +--> Response 3: \"345\"                   (correct)\n",
    "  +--> Response 4: \"Hmm, not sure\"         (no answer)\n",
    "\n",
    "PHASE 2: SCORING & ADVANTAGES\n",
    "  Rewards:    [+1.0,  -0.5,  +1.0,  -1.0]    (from reward_fn)\n",
    "  Baseline:   +0.125                           (mean)\n",
    "  Advantages: [+0.875, -0.625, +0.875, -1.125] (rewards - baseline)\n",
    "\n",
    "PHASE 3: LOSS COMPUTATION (with gradients!)\n",
    "  For each (response, advantage) pair:\n",
    "    loss -= advantage * log_prob / G\n",
    "  \n",
    "  Positive advantage --> loss becomes more negative --> gradient pushes UP log_prob\n",
    "  Negative advantage --> loss becomes more positive --> gradient pushes DOWN log_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Simulating GRPO\n",
    "\n",
    "Let's trace through the algorithm with concrete numbers (no model needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate a GRPO step with G=4\n",
    "G = 4\n",
    "\n",
    "# Simulated rewards for 4 responses\n",
    "rewards = torch.tensor([1.0, -0.5, 1.0, -1.0])\n",
    "response_descriptions = [\n",
    "    \"'The answer is 345'      (correct)\",\n",
    "    \"'I think it's 355'       (wrong answer)\",\n",
    "    \"'345'                    (correct)\",\n",
    "    \"'Hmm, not sure'          (no answer)\",\n",
    "]\n",
    "\n",
    "# Simulated log-probabilities (as if computed by compute_log_probs)\n",
    "log_probs = torch.tensor([-15.2, -18.7, -12.3, -22.1])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRPO Step Simulation\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPrompt: 'What is 15 * 23?'\")\n",
    "print(f\"Ground truth: 345\")\n",
    "print(f\"Group size G: {G}\")\n",
    "\n",
    "# Phase 1: Show responses and rewards\n",
    "print(f\"\\n--- Phase 1: Responses & Rewards ---\")\n",
    "for i in range(G):\n",
    "    print(f\"  Response {i+1}: {response_descriptions[i]}\")\n",
    "    print(f\"    Reward: {rewards[i]:+.1f}\")\n",
    "\n",
    "# Phase 2: Compute advantages\n",
    "baseline = rewards.mean()\n",
    "advantages = rewards - baseline\n",
    "\n",
    "print(f\"\\n--- Phase 2: Advantages ---\")\n",
    "print(f\"  Baseline (mean reward): {baseline:.4f}\")\n",
    "for i in range(G):\n",
    "    sign = \"+\" if advantages[i] > 0 else \"\"\n",
    "    print(f\"  Response {i+1}: advantage = {rewards[i]:+.1f} - {baseline:.4f} = {sign}{advantages[i]:.4f}\")\n",
    "\n",
    "# Phase 3: Compute loss\n",
    "loss = 0.0\n",
    "print(f\"\\n--- Phase 3: Loss Computation ---\")\n",
    "for i in range(G):\n",
    "    if abs(advantages[i]) < 1e-8:\n",
    "        print(f\"  Response {i+1}: advantage ~0, SKIPPED\")\n",
    "        continue\n",
    "    \n",
    "    contribution = -advantages[i].item() * log_probs[i].item() / G\n",
    "    loss += contribution\n",
    "    print(f\"  Response {i+1}: -({advantages[i]:+.4f}) * ({log_probs[i]:.1f}) / {G} = {contribution:+.4f}\")\n",
    "\n",
    "print(f\"\\n  Total loss: {loss:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Phase 4: Gradient Direction ---\")\n",
    "for i in range(G):\n",
    "    if advantages[i] > 0:\n",
    "        print(f\"  Response {i+1}: Positive advantage --> INCREASE probability\")\n",
    "    elif advantages[i] < 0:\n",
    "        print(f\"  Response {i+1}: Negative advantage --> DECREASE probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Why GRPO Works on a Single GPU\n",
    "\n",
    "### Memory Comparison\n",
    "\n",
    "| | PPO | GRPO |\n",
    "|---|---|---|\n",
    "| Policy model | 8B params (4-bit) | 8B params (4-bit) |\n",
    "| Critic model | 8B params (16-bit, ~16 GB) | **None** |\n",
    "| Critic optimizer | ~32 GB | **None** |\n",
    "| Total extra cost | ~48 GB | 0 GB |\n",
    "| Fits on 24 GB GPU? | No | **Yes** |\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "| Advantage | Disadvantage |\n",
    "|---|---|\n",
    "| No critic = no extra memory | Higher variance baseline (small G) |\n",
    "| Simpler code (no second optimizer) | More generation cost (G forward passes) |\n",
    "| Fresh baseline every step | New hyperparameter G to tune |\n",
    "| Stable training | Less sample-efficient than PPO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The Skip Condition\n",
    "\n",
    "```python\n",
    "if adv.abs() < 1e-8:\n",
    "    continue\n",
    "```\n",
    "\n",
    "### When does this trigger?\n",
    "\n",
    "When **all G responses get the same reward**. For example, if all 4 responses are correct (+1.0 each), then:\n",
    "\n",
    "```\n",
    "baseline = (1.0 + 1.0 + 1.0 + 1.0) / 4 = 1.0\n",
    "advantage_i = 1.0 - 1.0 = 0.0 for all i\n",
    "```\n",
    "\n",
    "**There's nothing to learn** when all responses are equally good (or equally bad). The group provides no relative signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: when skip triggers\n",
    "\n",
    "scenarios = [\n",
    "    (\"All correct\",    [1.0, 1.0, 1.0, 1.0]),\n",
    "    (\"All wrong\",      [-0.5, -0.5, -0.5, -0.5]),\n",
    "    (\"Mixed (useful)\", [1.0, -0.5, 1.0, -1.0]),\n",
    "    (\"3 correct, 1 wrong\", [1.0, 1.0, 1.0, -0.5]),\n",
    "]\n",
    "\n",
    "print(\"When does learning happen?\")\n",
    "print(\"=\" * 60)\n",
    "for label, rewards in scenarios:\n",
    "    mean_r = sum(rewards) / len(rewards)\n",
    "    advantages = [r - mean_r for r in rewards]\n",
    "    has_nonzero = any(abs(a) > 1e-8 for a in advantages)\n",
    "    \n",
    "    status = \"LEARNING\" if has_nonzero else \"SKIP (all same)\"\n",
    "    print(f\"\\n{label}: rewards={rewards}\")\n",
    "    print(f\"  Baseline: {mean_r:.4f}\")\n",
    "    print(f\"  Advantages: {[f'{a:+.4f}' for a in advantages]}\")\n",
    "    print(f\"  --> {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Choosing the Group Size G\n",
    "\n",
    "G is a critical hyperparameter that balances several factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Simulate how G affects baseline quality\n",
    "# True expected reward for this prompt: 0.3\n",
    "true_expected_reward = 0.3\n",
    "\n",
    "# Simulate many trials for each G value\n",
    "random.seed(42)\n",
    "n_trials = 1000\n",
    "\n",
    "print(\"Effect of G on baseline estimation quality\")\n",
    "print(\"(True expected reward = 0.3)\")\n",
    "print(f\"\\n{'G':<5} {'Mean baseline':<18} {'Std of baseline':<18} {'Variance reduction'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for G in [2, 4, 8, 16, 32]:\n",
    "    baselines = []\n",
    "    for _ in range(n_trials):\n",
    "        # Simulate G rewards drawn from some distribution\n",
    "        rewards = [random.choice([1.0, -0.5, -1.0]) for _ in range(G)]\n",
    "        baselines.append(sum(rewards) / len(rewards))\n",
    "    \n",
    "    import statistics\n",
    "    mean_bl = statistics.mean(baselines)\n",
    "    std_bl = statistics.stdev(baselines)\n",
    "    print(f\"{G:<5} {mean_bl:<18.4f} {std_bl:<18.4f} {'Low' if G <= 2 else 'Medium' if G <= 8 else 'High'}\")\n",
    "\n",
    "print(\"\\nThe book uses G=4 as a sweet spot:\")\n",
    "print(\"  - Enough diversity for meaningful advantages\")\n",
    "print(\"  - Not too many forward passes (4 generations per step)\")\n",
    "print(\"  - Fits in 24 GB GPU memory with Qwen3-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Advantage Normalization\n",
    "The current code uses raw advantages (`rewards - mean`). Many implementations also **normalize** advantages by dividing by the standard deviation: `(rewards - mean) / (std + epsilon)`. Implement this and think about when it would help vs hurt.\n",
    "\n",
    "### Exercise 2: KL Divergence Penalty\n",
    "The full GRPO paper includes a KL divergence penalty to prevent the model from drifting too far from the original pretrained model. This looks like: `loss += beta * KL(pi_current || pi_reference)`. Why might this be important? What happens without it?\n",
    "\n",
    "### Exercise 3: Varying Group Size\n",
    "Modify the simulation to show what happens with G=2 (very noisy baseline) vs G=16 (accurate but slow). At what point does increasing G have diminishing returns?\n",
    "\n",
    "### Exercise 4: Understanding the Loss Sign\n",
    "Trace through the math: if advantage is +0.5 and log_prob is -15.0, what is the contribution to the loss? What direction will the optimizer move? What about advantage -0.5 and log_prob -15.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The log-derivative trick** lets us compute gradients even though sampling is non-differentiable. We compute the log-probability of the sampled response and multiply by the reward.\n",
    "\n",
    "2. **Baselines reduce variance** by turning raw rewards into advantages. Only responses better than average get encouraged; worse than average get discouraged.\n",
    "\n",
    "3. **GRPO uses the group mean as the baseline**, eliminating the need for a critic network. This saves ~48 GB of memory.\n",
    "\n",
    "4. **The algorithm has two gradient modes**: Generation (no_grad, inference only) and loss computation (with grad, differentiable).\n",
    "\n",
    "5. **G=4 is the sweet spot** for single-GPU training: enough diversity for meaningful learning, not too expensive in forward passes.\n",
    "\n",
    "6. **When all responses get the same reward**, there's nothing to learn - the step is skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Previous:** [Chapter 3 - Reward Signals](../ch03_rewards/learn_rewards.ipynb)  \n",
    "**Next:** [Chapter 7 - Training Loop](../ch07_training/learn_training.ipynb) - How do we put all the pieces together into a complete training script?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}