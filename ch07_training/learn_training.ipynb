{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapters 6, 7 & 8: Think Tokens, Training Loop & Evaluation\n",
    "\n",
    "**Goal:** Understand how all the pieces (rollouts, rewards, GRPO) come together in a complete training loop, how think tokens teach the model to reason, and how to evaluate the results.\n",
    "\n",
    "This notebook covers:\n",
    "- **Chapter 6:** Think tokens and emergent reasoning behavior\n",
    "- **Chapter 7:** The complete training loop in `train.py`\n",
    "- **Chapter 8:** Evaluation benchmarks and next steps\n",
    "\n",
    "---\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Everything we've built so far:\n",
    "\n",
    "```\n",
    "Ch 2: load_model_qlora()     --> Load 8B model in ~5.7 GB\n",
    "       sample_batch()         --> Generate diverse responses\n",
    "       \n",
    "Ch 3: math_reward()           --> Score responses (+1.0, -0.5, -1.0)\n",
    "       reward_with_thinking() --> Also reward <think> blocks\n",
    "       \n",
    "Ch 5: grpo_step()             --> One RL update (generate G responses, compute advantages, backprop)\n",
    "\n",
    "Ch 7: train()                 --> Tie it all together in a loop!\n",
    "       |\n",
    "       +-- for step in range(num_steps):\n",
    "       |     1. Get prompt from GSM8K dataset\n",
    "       |     2. optimizer.zero_grad()\n",
    "       |     3. grpo_step() --> generates, scores, computes loss, backward()\n",
    "       |     4. optimizer.step() --> update LoRA weights\n",
    "       |     5. torch.cuda.empty_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Think Tokens (Chapter 6)\n",
    "\n",
    "### What are think tokens?\n",
    "\n",
    "Special `<think>...</think>` blocks where the model externalizes its reasoning before giving a final answer. During inference, the think block can be stripped - the user only sees the final answer.\n",
    "\n",
    "### Example: Math Problem\n",
    "\n",
    "**Without thinking:**\n",
    "```\n",
    "Q: What is 127 * 843?\n",
    "A: 107,061\n",
    "```\n",
    "\n",
    "**With thinking:**\n",
    "```\n",
    "Q: What is 127 * 843?\n",
    "<think>\n",
    "I need to compute 127 * 843.\n",
    "Let me break this down:\n",
    "127 * 800 = 101,600\n",
    "127 * 40  = 5,080\n",
    "127 * 3   = 381\n",
    "\n",
    "Total: 101,600 + 5,080 + 381 = 107,061\n",
    "\n",
    "Let me verify: 107,061 / 127 = 843. Correct.\n",
    "</think>\n",
    "The answer is 107,061.\n",
    "```\n",
    "\n",
    "### Why is this valuable?\n",
    "\n",
    "| Benefit | Explanation |\n",
    "|---|---|\n",
    "| Problem decomposition | Forces multi-step breakdown |\n",
    "| Credit assignment | Reward function can score intermediate steps |\n",
    "| Interpretability | See exactly where errors occur |\n",
    "| Self-correction | Model can review its own work |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the model learns WHEN to think\n",
    "\n",
    "The model is **never explicitly told** which problems are hard. It learns this emergently through GRPO's relative scoring:\n",
    "\n",
    "**Simple problem (2 + 3):**\n",
    "- Response without thinking: \"5\" --> reward +1.0 (or +0.5 with thinking reward)\n",
    "- Response with thinking: \"<think>2+3=5</think> 5\" --> reward +1.0\n",
    "- Within the GRPO group, non-thinking and thinking responses get similar rewards\n",
    "- The model learns: thinking is unnecessary overhead for trivial problems\n",
    "\n",
    "**Complex problem (127 * 843):**\n",
    "- Response without thinking: often wrong --> reward -0.5\n",
    "- Response with thinking: step-by-step, often correct --> reward +1.0\n",
    "- Within the GRPO group, thinking responses dominate\n",
    "- The model learns: thinking is essential for hard problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating how GRPO learns when to think\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def simulate_grpo_group(problem_difficulty: str, G: int = 4):\n",
    "    \"\"\"Simulate a GRPO group for a simple vs complex problem.\"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for _ in range(G):\n",
    "        uses_thinking = random.random() < 0.5  # 50% chance of thinking\n",
    "        \n",
    "        if problem_difficulty == \"simple\":\n",
    "            # Simple problems: usually correct regardless of thinking\n",
    "            correct = random.random() < 0.95  # 95% accuracy\n",
    "        else:\n",
    "            # Complex problems: thinking helps a lot\n",
    "            if uses_thinking:\n",
    "                correct = random.random() < 0.75  # 75% with thinking\n",
    "            else:\n",
    "                correct = random.random() < 0.20  # 20% without thinking\n",
    "        \n",
    "        # reward_with_thinking scoring\n",
    "        if correct and uses_thinking:\n",
    "            reward = 1.0\n",
    "        elif correct:\n",
    "            reward = 0.5\n",
    "        elif uses_thinking:\n",
    "            reward = -0.2\n",
    "        else:\n",
    "            reward = -0.5\n",
    "        \n",
    "        responses.append((uses_thinking, correct, reward))\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "# Run many simulations\n",
    "n_sims = 5000\n",
    "\n",
    "for difficulty in [\"simple\", \"complex\"]:\n",
    "    thinking_encouraged = 0\n",
    "    not_thinking_encouraged = 0\n",
    "    \n",
    "    for _ in range(n_sims):\n",
    "        group = simulate_grpo_group(difficulty)\n",
    "        rewards = [r for _, _, r in group]\n",
    "        baseline = sum(rewards) / len(rewards)\n",
    "        \n",
    "        for thinks, correct, reward in group:\n",
    "            advantage = reward - baseline\n",
    "            if advantage > 0.01:\n",
    "                if thinks:\n",
    "                    thinking_encouraged += 1\n",
    "                else:\n",
    "                    not_thinking_encouraged += 1\n",
    "    \n",
    "    total = thinking_encouraged + not_thinking_encouraged\n",
    "    if total > 0:\n",
    "        think_pct = 100 * thinking_encouraged / total\n",
    "        no_think_pct = 100 * not_thinking_encouraged / total\n",
    "    else:\n",
    "        think_pct = no_think_pct = 0\n",
    "    \n",
    "    print(f\"\\n{difficulty.upper()} problems:\")\n",
    "    print(f\"  Thinking responses encouraged:     {think_pct:.1f}%\")\n",
    "    print(f\"  Non-thinking responses encouraged:  {no_think_pct:.1f}%\")\n",
    "\n",
    "print(\"\\nConclusion: GRPO naturally learns to think more on complex problems!\")\n",
    "print(\"This is EMERGENT behavior - no explicit 'difficulty detector' is needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The GSM8K Dataset\n",
    "\n",
    "The training loop uses **GSM8K** (Grade School Math 8K) - a dataset of 8,500 multi-step grade school math word problems.\n",
    "\n",
    "### Dataset format\n",
    "\n",
    "Each example has:\n",
    "- `question`: A math word problem in natural language\n",
    "- `answer`: A step-by-step solution ending with `#### <number>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the GSM8K answer format\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract the final numerical answer from GSM8K's answer format.\n",
    "    \n",
    "    GSM8K answers look like:\n",
    "    'Step 1: Calculate X = 5 * 3 = 15\n",
    "     Step 2: Calculate Y = 15 + 7 = 22\n",
    "     #### 22'\n",
    "    \n",
    "    We need to extract '22' from after the '####'.\n",
    "    \"\"\"\n",
    "    lines = answer_text.strip().split(\"\\n\")\n",
    "    # Search from the END of the text for the #### marker\n",
    "    for line in reversed(lines):\n",
    "        if \"####\" in line:\n",
    "            return line.split(\"####\")[-1].strip()\n",
    "    # Fallback: return the last line\n",
    "    return lines[-1].strip() if lines else \"\"\n",
    "\n",
    "\n",
    "# Test with realistic GSM8K examples\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells every duck egg at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "        \"answer\": \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = <<9*2=18>>$18 every day at the farmer's market.\\n#### 18\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\",\n",
    "        \"answer\": \"It takes 2/2=<<2/2=1>>1 bolt of white fiber.\\nSo the total bolts needed is 2+1=<<2+1=3>>3.\\n#### 3\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"GSM8K Dataset Examples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, ex in enumerate(examples):\n",
    "    answer = extract_gsm8k_answer(ex[\"answer\"])\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Question: {ex['question'][:80]}...\")\n",
    "    print(f\"  Full answer: {ex['answer'][:80]}...\")\n",
    "    print(f\"  Extracted ground truth: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Training Loop - `train()` Line by Line\n",
    "\n",
    "Now let's walk through the complete training function from `train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotated version of the train() function\n",
    "# (This is a walkthrough - don't run this cell unless you have a GPU)\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# These imports show how the chapters connect:\n",
    "# from ch02_rollouts.sample import load_model_qlora      # Chapter 2: Model loading\n",
    "# from ch03_rewards.verifier import math_reward, reward_with_thinking  # Chapter 3: Rewards\n",
    "# from ch05_grpo.grpo import grpo_step                   # Chapter 5: GRPO algorithm\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_steps: int = 50,              # Total training steps\n",
    "    G: int = 4,                        # GRPO group size\n",
    "    lr: float = 1e-5,                  # Learning rate\n",
    "    use_thinking_reward: bool = False,  # Use reward_with_thinking?\n",
    "    log_interval: int = 5,             # Print progress every N steps\n",
    "):\n",
    "    print(\"=== GRPO Training on RTX 3090 ===\")\n",
    "    print(f\"Steps: {num_steps}, G: {G}, LR: {lr}\")\n",
    "    print(f\"Thinking reward: {use_thinking_reward}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # SETUP PHASE\n",
    "    # ========================================\n",
    "    \n",
    "    # Step 1: Load model with QLoRA (Chapter 2)\n",
    "    # This gives us ~5.7 GB base model + ~0.2 GB trainable LoRA adapters\n",
    "    # model, tokenizer = load_model_qlora()\n",
    "    \n",
    "    # Step 2: Set up optimizer\n",
    "    # AdamW = Adam with weight decay. Only updates LoRA parameters.\n",
    "    # lr=1e-5 is conservative - prevents catastrophic forgetting\n",
    "    # optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Step 3: Load dataset\n",
    "    # GSM8K: 8,500 grade school math problems\n",
    "    # Shuffle with fixed seed for reproducibility\n",
    "    # dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "    # dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Step 4: Select reward function\n",
    "    # math_reward: only checks final answer\n",
    "    # reward_with_thinking: also rewards <think> blocks\n",
    "    # reward_fn = reward_with_thinking if use_thinking_reward else math_reward\n",
    "    \n",
    "    print(\"\\nSetup complete. Starting training loop...\")\n",
    "    print(\"(This is a walkthrough - not actually running)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core training loop (annotated walkthrough)\n",
    "\n",
    "print(\"THE CORE TRAINING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "for step in range(num_steps):  # e.g., 50 steps\n",
    "    \n",
    "    # --- Get training example ---\n",
    "    example = dataset[step % len(dataset)]   # Cycle through dataset\n",
    "    prompt = example[\"question\"]              # \"Janet's ducks lay 16 eggs...\"\n",
    "    ground_truth = extract_gsm8k_answer(      # \"18\"\n",
    "        example[\"answer\"]\n",
    "    )\n",
    "    \n",
    "    # --- GRPO Step ---\n",
    "    optimizer.zero_grad()                     # Clear gradients from last step\n",
    "    \n",
    "    loss, reward, responses = grpo_step(       # THE MAIN EVENT:\n",
    "        model, tokenizer,                      #   1. Generate G=4 responses\n",
    "        prompt, ground_truth,                  #   2. Score each with reward_fn\n",
    "        reward_fn, G=G                         #   3. Compute advantages\n",
    "    )                                          #   4. Compute loss & backward()\n",
    "    \n",
    "    optimizer.step()                           # Update LoRA weights!\n",
    "    \n",
    "    # --- Memory management ---\n",
    "    torch.cuda.empty_cache()                   # Free unused GPU memory\n",
    "                                               # Prevents fragmentation\n",
    "\"\"\")\n",
    "\n",
    "print(\"Each step:\")\n",
    "print(\"  1. Pick a math problem from GSM8K\")\n",
    "print(\"  2. Generate 4 different responses\")\n",
    "print(\"  3. Score them (correct=+1.0, wrong=-0.5, no answer=-1.0)\")\n",
    "print(\"  4. Compute advantages (each reward minus group mean)\")\n",
    "print(\"  5. Compute loss: -sum(advantage * log_prob) / G\")\n",
    "print(\"  6. Backpropagate through the LoRA parameters\")\n",
    "print(\"  7. Optimizer updates the LoRA weights\")\n",
    "print(\"  8. Clear GPU cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Memory Budget on RTX 3090\n",
    "\n",
    "One of the most practical aspects of this system is fitting everything into 24 GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory budget breakdown\n",
    "\n",
    "components = [\n",
    "    (\"Base model (4-bit NF4)\",     5.0,  \"Frozen, not trainable\"),\n",
    "    (\"LoRA adapters (float32)\",    0.2,  \"~50M trainable parameters\"),\n",
    "    (\"Optimizer states (AdamW)\",   0.4,  \"2x LoRA params (m and v)\"),\n",
    "    (\"Gradients\",                  0.2,  \"Same size as LoRA params\"),\n",
    "    (\"Activations (peak)\",         6.0,  \"During forward/backward pass\"),\n",
    "    (\"KV cache (generation)\",      1.5,  \"For G=4 sequential generations\"),\n",
    "    (\"CUDA overhead\",              0.5,  \"Context, allocator, etc.\"),\n",
    "]\n",
    "\n",
    "total_gpu = 24.0\n",
    "total_used = sum(gb for _, gb, _ in components)\n",
    "\n",
    "print(\"Memory Budget: GRPO Training on RTX 3090 (24 GB)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Component':<30} {'GB':<8} {'Notes'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, gb, notes in components:\n",
    "    bar = \"#\" * int(gb * 2)\n",
    "    print(f\"{name:<30} {gb:<8.1f} {bar} {notes}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'TOTAL USED':<30} {total_used:<8.1f}\")\n",
    "print(f\"{'AVAILABLE':<30} {total_gpu:<8.1f}\")\n",
    "print(f\"{'HEADROOM':<30} {total_gpu - total_used:<8.1f}\")\n",
    "print()\n",
    "print(f\"Peak usage from book: ~13.57 GB (well within 24 GB budget)\")\n",
    "print(f\"Headroom allows for longer sequences or larger G if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key memory management strategies\n",
    "\n",
    "1. **QLoRA** reduces model from 16 GB to ~5 GB (the big win)\n",
    "2. **Small G=4** limits concurrent forward passes\n",
    "3. **Sequential generation** (one response at a time, not batched)\n",
    "4. **`torch.cuda.empty_cache()`** prevents memory fragmentation between steps\n",
    "5. **Only LoRA parameters** have gradients, optimizer states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Understanding the Optimizer\n",
    "\n",
    "The training loop uses **AdamW** with `lr=1e-5`. Let's understand why these choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why lr=1e-5 and not larger?\n",
    "\n",
    "print(\"Learning Rate Selection: lr = 1e-5\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Why so small?\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. We're fine-tuning a PRETRAINED model\")\n",
    "print(\"   - Too large LR would destroy pretrained knowledge\")\n",
    "print(\"   - This is called 'catastrophic forgetting'\")\n",
    "print()\n",
    "print(\"2. RL gradients are inherently noisy\")\n",
    "print(\"   - Based on sampled responses (not ground truth)\")\n",
    "print(\"   - Small G=4 means noisy advantage estimates\")\n",
    "print(\"   - Small LR smooths out the noise\")\n",
    "print()\n",
    "print(\"3. LoRA alpha/r scaling already amplifies updates\")\n",
    "print(\"   - alpha=16, r=64 --> scaling = 0.25\")\n",
    "print(\"   - Effective LR for LoRA = 1e-5 * 0.25 = 2.5e-6\")\n",
    "print()\n",
    "print(\"Why AdamW and not plain SGD?\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Adam's adaptive learning rate per-parameter\")\n",
    "print(\"   helps with the high-variance RL gradients\")\n",
    "print(\"2. Weight decay (the 'W' in AdamW) provides\")\n",
    "print(\"   mild regularization, preventing LoRA params\")\n",
    "print(\"   from growing too large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Command Line Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the CLI arguments from train.py\n",
    "\n",
    "print(\"Running train.py from the command line:\")\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "print(\"Basic run (default settings):\")\n",
    "print(\"  python ch07_training/train.py\")\n",
    "print(\"  --> 50 steps, G=4, lr=1e-5, math_reward\")\n",
    "print()\n",
    "print(\"With thinking reward:\")\n",
    "print(\"  python ch07_training/train.py --thinking\")\n",
    "print(\"  --> Same but uses reward_with_thinking\")\n",
    "print()\n",
    "print(\"Custom configuration:\")\n",
    "print(\"  python ch07_training/train.py --steps 100 --G 8 --lr 2e-5 --thinking\")\n",
    "print(\"  --> 100 steps, G=8, lr=2e-5, reward_with_thinking\")\n",
    "print()\n",
    "print(\"Arguments:\")\n",
    "print(f\"  {'--steps':<12} Number of training steps (default: 50)\")\n",
    "print(f\"  {'--G':<12} GRPO group size (default: 4)\")\n",
    "print(f\"  {'--lr':<12} Learning rate (default: 1e-5)\")\n",
    "print(f\"  {'--thinking':<12} Use thinking reward (default: off)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: What Training Looks Like\n",
    "\n",
    "The book provides simulated training output. Let's understand what the metrics mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating training progress\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "print(\"=== GRPO Training on RTX 3090 ===\")\n",
    "print(\"Steps: 50, G: 4, LR: 1e-05\")\n",
    "print(\"Thinking reward: True\")\n",
    "print()\n",
    "\n",
    "# Simulate improving metrics over training\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "for step in range(50):\n",
    "    # Simulate gradually improving reward\n",
    "    base_reward = -0.2 + (0.8 * step / 50)  # -0.2 to +0.6 over training\n",
    "    reward = base_reward + random.uniform(-0.3, 0.3)\n",
    "    reward = max(-1.0, min(1.0, reward))  # Clamp\n",
    "    \n",
    "    loss = -0.05 - (0.3 * step / 50) + random.uniform(-0.1, 0.1)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        avg_loss = sum(losses[-10:]) / 10\n",
    "        avg_reward = sum(rewards[-10:]) / 10\n",
    "        elapsed = (step + 1) * 3.0  # ~3 sec per step\n",
    "        print(f\"Step {step+1:>3}: loss={avg_loss:>8.4f}, reward={avg_reward:>7.4f}, time={elapsed:.1f}s\")\n",
    "\n",
    "total_time = 50 * 3.0\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Total time: {total_time:.1f}s ({total_time/50:.2f}s/step)\")\n",
    "print(f\"Final avg loss: {sum(losses[-10:])/10:.4f}\")\n",
    "print(f\"Final avg reward: {sum(rewards[-10:])/10:.4f}\")\n",
    "print(f\"Peak GPU memory: ~13.57 GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the metrics\n",
    "\n",
    "| Metric | What it means | Good trend |\n",
    "|---|---|---|\n",
    "| **loss** | GRPO policy gradient loss | Becomes more negative (model is learning) |\n",
    "| **reward** | Average reward across G responses | Increases toward +1.0 |\n",
    "| **time** | Wall clock time | Stable per-step time |\n",
    "| **Peak GPU memory** | Maximum VRAM usage | Should stay under 24 GB |\n",
    "\n",
    "### What to watch for\n",
    "- **Reward plateaus at 0.0:** All responses get same score (all correct or all wrong). Increase G or adjust temperature.\n",
    "- **Loss oscillates wildly:** LR too high. Reduce it.\n",
    "- **OOM errors:** Reduce G or max_new_tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Results (Chapter 7)\n",
    "\n",
    "The book reports preliminary results on GSM8K (100 held-out test problems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from the book\n",
    "\n",
    "results = [\n",
    "    (\"Qwen3-8B-Instruct (Base)\",    58, 5),\n",
    "    (\"GRPO + math_reward\",           67, 12),\n",
    "    (\"GRPO + reward_with_thinking\",  71, 85),\n",
    "]\n",
    "\n",
    "print(\"Preliminary Results on GSM8K (100 test problems)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Model':<35} {'Accuracy':<12} {'Think Block %'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for model_name, accuracy, think_pct in results:\n",
    "    acc_bar = \"*\" * (accuracy // 2)\n",
    "    print(f\"{model_name:<35} {accuracy}%{'':<6} {think_pct}%\")\n",
    "\n",
    "print()\n",
    "print(\"Key findings:\")\n",
    "print(\"  1. GRPO alone improved accuracy by +9% (58% -> 67%)\")\n",
    "print(\"  2. Adding thinking reward gave +13% total (58% -> 71%)\")\n",
    "print(\"  3. Think block presence jumped from 5% to 85%!\")\n",
    "print(\"  4. The model learned to REASON, not just memorize answers\")\n",
    "print()\n",
    "print(\"The +4% gap between math_reward and reward_with_thinking\")\n",
    "print(\"shows that explicit reasoning traces help the model solve\")\n",
    "print(\"problems it would otherwise get wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Evaluation & Benchmarks (Chapter 8)\n",
    "\n",
    "### Three key benchmarks\n",
    "\n",
    "| Benchmark | Task | Size | Metric | Difficulty |\n",
    "|---|---|---|---|---|\n",
    "| **GSM8K** | Grade school math | 8,500 | Numerical accuracy | Medium |\n",
    "| **MATH** | Competition math | 12,500 | Accuracy (5 subjects) | Hard |\n",
    "| **HumanEval** | Code generation | 164 | pass@k | Variable |\n",
    "\n",
    "### The evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How evaluation works (conceptual)\n",
    "\n",
    "print(\"Evaluation Loop (pseudo-code):\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "def evaluate(model, tokenizer, dataset, reward_fn):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for example in dataset:\n",
    "        # Generate with LOW temperature (near-deterministic)\n",
    "        response = sample_response(\n",
    "            model, tokenizer,\n",
    "            example[\"question\"],\n",
    "            temperature=0.1  # <-- Key difference from training!\n",
    "        )\n",
    "        \n",
    "        # Check correctness\n",
    "        ground_truth = extract_gsm8k_answer(example[\"answer\"])\n",
    "        reward = reward_fn(response, ground_truth)\n",
    "        \n",
    "        if reward > 0:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\"\"\")\n",
    "\n",
    "print(\"Critical difference: Training uses T=0.7, Evaluation uses T=0.1\")\n",
    "print(\"  Training: Need diversity for exploration\")\n",
    "print(\"  Evaluation: Want the model's best answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative analysis\n",
    "\n",
    "Beyond accuracy numbers, the book recommends inspecting:\n",
    "\n",
    "1. **Error analysis:** Look at think blocks for wrong answers. Is it:\n",
    "   - An arithmetic mistake?\n",
    "   - A misunderstanding of the problem?\n",
    "   - A logic error?\n",
    "\n",
    "2. **Trace consistency:** Does the final answer logically follow from the think block? A common failure mode is correct reasoning but a wrong final number.\n",
    "\n",
    "3. **Success analysis:** Are successful reasoning traces elegant and efficient, or convoluted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Future Directions (Chapter 8)\n",
    "\n",
    "The book outlines four directions that go beyond the single-GPU setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_directions = [\n",
    "    {\n",
    "        \"name\": \"Full Fine-Tuning\",\n",
    "        \"description\": \"Update ALL weights, not just LoRA adapters\",\n",
    "        \"requirement\": \"Multiple A100/H100 GPUs + DeepSpeed/FSDP\",\n",
    "        \"benefit\": \"Deeper integration of reasoning capabilities\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Process Reward Models (PRMs)\",\n",
    "        \"description\": \"Score every individual reasoning step, not just the final answer\",\n",
    "        \"requirement\": \"Powerful model (e.g., GPT-4) to evaluate each step\",\n",
    "        \"benefit\": \"Much more granular training signal\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Monte Carlo Tree Search (MCTS)\",\n",
    "        \"description\": \"Build a search tree of reasoning paths at inference time\",\n",
    "        \"requirement\": \"Significant inference-time compute\",\n",
    "        \"benefit\": \"Explore multiple reasoning strategies, pick the best\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Distillation\",\n",
    "        \"description\": \"Use a large reasoning model to generate traces, train a smaller model to mimic\",\n",
    "        \"requirement\": \"A powerful 'teacher' model\",\n",
    "        \"benefit\": \"Deploy reasoning in smaller, faster models\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Future Directions (Beyond Single GPU)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, d in enumerate(future_directions, 1):\n",
    "    print(f\"\\n{i}. {d['name']}\")\n",
    "    print(f\"   What: {d['description']}\")\n",
    "    print(f\"   Needs: {d['requirement']}\")\n",
    "    print(f\"   Gains: {d['benefit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Complete Data Flow Diagram\n",
    "\n",
    "Let's trace a single training step end-to-end to solidify understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "COMPLETE DATA FLOW: One Training Step\n",
    "======================================\n",
    "\n",
    "1. DATASET --> Prompt\n",
    "   GSM8K[step=7] --> \"Janet's ducks lay 16 eggs per day...\"\n",
    "   extract_gsm8k_answer() --> ground_truth = \"18\"\n",
    "\n",
    "2. FORMATTING (ch02)\n",
    "   apply_chat_template() --> \"<|im_start|>user\\nJanet's ducks...\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "3. GENERATION (ch02, torch.no_grad)\n",
    "   model.generate() x4 -->\n",
    "     R1: \"She sells 16-3-4=9 eggs. 9*2=$18. The answer is 18\"\n",
    "     R2: \"16 eggs minus 3 minus 4 is 9. 9 times 2 is 20. Answer: 20\"\n",
    "     R3: \"<think>16-3-4=9 eggs sold. 9*$2=$18</think> The answer is 18\"\n",
    "     R4: \"I think she makes about $15\"\n",
    "\n",
    "4. SCORING (ch03)\n",
    "   reward_with_thinking():\n",
    "     R1: correct, no think  --> +0.5\n",
    "     R2: wrong, no think    --> -0.5\n",
    "     R3: correct + think    --> +1.0\n",
    "     R4: wrong, no think    --> -0.5\n",
    "\n",
    "5. ADVANTAGES (ch05)\n",
    "   baseline = mean(+0.5, -0.5, +1.0, -0.5) = +0.125\n",
    "   A1 = +0.5 - 0.125  = +0.375  (encourage)\n",
    "   A2 = -0.5 - 0.125  = -0.625  (discourage)\n",
    "   A3 = +1.0 - 0.125  = +0.875  (STRONGLY encourage)\n",
    "   A4 = -0.5 - 0.125  = -0.625  (discourage)\n",
    "\n",
    "6. LOSS (ch05)\n",
    "   For each response with non-zero advantage:\n",
    "     loss -= advantage * log_prob(response | prompt) / G\n",
    "   loss.backward()  --> gradients flow through LoRA adapters\n",
    "\n",
    "7. UPDATE (ch07)\n",
    "   optimizer.step()  --> AdamW updates LoRA weights\n",
    "   \n",
    "   Net effect after this step:\n",
    "     - Model is slightly more likely to produce R3-style responses\n",
    "       (correct + thinking = highest advantage)\n",
    "     - Model is slightly less likely to produce R2 and R4-style responses\n",
    "       (wrong answers = negative advantages)\n",
    "\n",
    "8. CLEANUP\n",
    "   torch.cuda.empty_cache()  --> free GPU memory\n",
    "   --> Ready for next step\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Logging Enhancements\n",
    "The current training loop logs loss and reward. Add logging for:\n",
    "- Percentage of responses with `<think>` blocks\n",
    "- Average response length (in tokens)\n",
    "- Per-step timing breakdown (generation vs scoring vs backprop)\n",
    "\n",
    "### Exercise 2: Gradient Accumulation\n",
    "The book mentions gradient accumulation as an extension. Modify the training loop to accumulate gradients over `K` steps before calling `optimizer.step()`. This simulates a larger effective batch size.\n",
    "\n",
    "### Exercise 3: Learning Rate Schedule\n",
    "Implement a simple warmup + cosine decay learning rate schedule. Why might this be important for RL training stability?\n",
    "\n",
    "### Exercise 4: Evaluation Script\n",
    "Write a function that evaluates the model on a held-out set of GSM8K problems. Compare accuracy before and after training.\n",
    "\n",
    "### Exercise 5: Curriculum Learning\n",
    "Instead of random sampling from GSM8K, implement a curriculum that starts with simpler problems and gradually introduces harder ones. How would you measure problem difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Think tokens** teach the model to externalize reasoning. The model learns WHEN to think through GRPO's relative scoring - no explicit difficulty labels needed.\n",
    "\n",
    "2. **The training loop** is deceptively simple: `zero_grad --> grpo_step --> optimizer.step --> empty_cache`. All the complexity is inside `grpo_step`.\n",
    "\n",
    "3. **Memory fits in 24 GB** thanks to QLoRA (4-bit base + small LoRA adapters) and sequential generation (one response at a time).\n",
    "\n",
    "4. **GSM8K results** show +13% accuracy improvement with thinking rewards, and the model spontaneously adopts reasoning traces in 85% of responses.\n",
    "\n",
    "5. **Evaluation uses low temperature** (T=0.1) to get the model's best answer, unlike training which uses T=0.7 for exploration.\n",
    "\n",
    "6. **This is just the beginning** - full fine-tuning, process reward models, MCTS, and distillation can push results further with more compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Previous:** [Chapter 5 - GRPO](../ch05_grpo/learn_grpo.ipynb)  \n",
    "\n",
    "**Congratulations!** You've now walked through the entire RL Post-Training Handbook codebase. You understand:\n",
    "- How to load and efficiently fine-tune large models (QLoRA)\n",
    "- How to generate diverse training data (rollouts with temperature)\n",
    "- How to design reward functions (verifiable rewards + think bonuses)\n",
    "- How GRPO eliminates the need for a critic (group-relative advantages)\n",
    "- How to assemble everything into a working training loop\n",
    "- How to evaluate and iterate on the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}