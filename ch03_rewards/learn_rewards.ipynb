{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Reward Signals\n",
    "\n",
    "**Goal:** Understand how to programmatically judge whether a model's output is good, and how reward design shapes what the model learns.\n",
    "\n",
    "This notebook breaks down every function in `verifier.py` and connects it to the book's explanations.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Flavors of Reward\n",
    "\n",
    "The book describes two fundamentally different approaches:\n",
    "\n",
    "| | Verifiable Rewards | Learned Rewards (Reward Models) |\n",
    "|---|---|---|\n",
    "| **What** | Deterministic functions | Separate ML model |\n",
    "| **Examples** | Math correctness, code execution, unit tests | Human preference prediction |\n",
    "| **Pros** | Cheap, fast, no false positives | Handles subjective tasks |\n",
    "| **Cons** | Only works for objective tasks | Expensive to train, can hallucinate |\n",
    "| **Used here** | **Yes** | No (too expensive for single GPU) |\n",
    "\n",
    "This chapter focuses entirely on **verifiable rewards** - functions that return a scalar score based on objective correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Answer Extraction - `extract_answer()`\n",
    "\n",
    "Before we can check if an answer is correct, we need to **find** the answer in the model's free-form text response.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "The model might express the same answer in many ways:\n",
    "- \"The answer is 42\"\n",
    "- \"So we get 6 * 7 = 42\"\n",
    "- \"\\\\boxed{42}\"\n",
    "- \"...calculating step by step... 42\"\n",
    "\n",
    "We need regex patterns that handle all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response: str) -> str | None:\n",
    "    \"\"\"Extract a numerical answer from a free-form text response.\n",
    "    \n",
    "    Uses a cascade of regex patterns, ordered from MOST SPECIFIC to LEAST SPECIFIC.\n",
    "    Returns the first match found, or None if no answer is detected.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        # Pattern 1: Keyword-based (most reliable)\n",
    "        # Matches: \"answer is 42\", \"result: 42\", \"equals 42\"\n",
    "        r\"(?:answer|result|equals?|is)[:\\s]+(-?\\d+(?:\\.\\d+)?)\",\n",
    "        \n",
    "        # Pattern 2: Equals sign at end of line\n",
    "        # Matches: \"6 * 7 = 42\" (common in step-by-step math)\n",
    "        r\"=\\s*(-?\\d+(?:\\.\\d+)?)\\s*$\",\n",
    "        \n",
    "        # Pattern 3: LaTeX boxed format\n",
    "        # Matches: \"\\boxed{42}\" (standard math competition format)\n",
    "        r\"\\\\boxed\\{(-?\\d+(?:\\.\\d+)?)}\",\n",
    "        \n",
    "        # Pattern 4: Last number in the response (fallback)\n",
    "        # Matches any number at the end of the text\n",
    "        # This is the least reliable - used only when nothing else matches\n",
    "        r\"(\\d+(?:\\.\\d+)?)\\s*$\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Let's test all the patterns:\n",
    "test_cases = [\n",
    "    (\"The answer is 42\",                    \"42\",   \"Pattern 1: keyword 'answer is'\"),\n",
    "    (\"result: 3.14\",                         \"3.14\", \"Pattern 1: keyword 'result:'\"),\n",
    "    (\"Let me calculate... 6 * 7 = 42\",      \"42\",   \"Pattern 2: equals at end\"),\n",
    "    (\"Therefore, \\\\boxed{42}\",               \"42\",   \"Pattern 3: LaTeX boxed\"),\n",
    "    (\"I computed each step and got 42\",      \"42\",   \"Pattern 4: last number (fallback)\"),\n",
    "    (\"I have no idea about this problem\",   None,    \"No match: returns None\"),\n",
    "    (\"The answer is -7.5\",                   \"-7.5\", \"Pattern 1: negative decimal\"),\n",
    "]\n",
    "\n",
    "print(\"Testing extract_answer():\")\n",
    "print(f\"{'Response':<45} {'Expected':<10} {'Got':<10} {'Pattern':<30}\")\n",
    "print(\"-\" * 95)\n",
    "for response, expected, pattern_name in test_cases:\n",
    "    result = extract_answer(response)\n",
    "    status = \"pass\" if result == expected else \"FAIL\"\n",
    "    print(f\"{response:<45} {str(expected):<10} {str(result):<10} {pattern_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the ordering matters\n",
    "\n",
    "Consider: \"The answer is 5, but let me verify: 2 + 3 = 5\"\n",
    "\n",
    "- Pattern 1 matches \"answer is 5\" --> returns \"5\" (correct!)\n",
    "- If we tried Pattern 4 first, it would also find \"5\" but for the wrong reason\n",
    "- If the response was \"The answer is 5, wait no, = 6\" and we used Pattern 2 first, we'd get \"6\" (wrong!)\n",
    "\n",
    "The cascade from most-specific to least-specific minimizes extraction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Math Reward - `math_reward()`\n",
    "\n",
    "### The Three-Tier Scoring System\n",
    "\n",
    "The book uses an **asymmetric reward scheme** that treats different failure modes differently:\n",
    "\n",
    "```\n",
    "+1.0  Correct answer    (best outcome)\n",
    "-0.5  Wrong answer      (the model tried but failed)\n",
    "-1.0  No answer found   (worst - model didn't even produce a parseable number)\n",
    "```\n",
    "\n",
    "### Why the asymmetry?\n",
    "\n",
    "- **-1.0 for no answer** is the strongest penalty because it means the model produced unparseable output. We want to **strongly discourage** incoherent responses.\n",
    "- **-0.5 for wrong answer** is milder because at least the model produced a number. This is a step in the right direction and shouldn't be punished as harshly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"Score a response against a known correct answer.\n",
    "    \n",
    "    Returns:\n",
    "        +1.0  if extracted answer matches ground truth\n",
    "        -0.5  if an answer was found but it's wrong\n",
    "        -1.0  if no answer could be extracted at all\n",
    "    \"\"\"\n",
    "    answer = extract_answer(response)\n",
    "    \n",
    "    if answer is None:\n",
    "        return -1.0  # No parseable answer --> strongest penalty\n",
    "    \n",
    "    try:\n",
    "        # Numeric comparison with tolerance (handles floating point)\n",
    "        if abs(float(answer) - float(ground_truth)) < 1e-6:\n",
    "            return 1.0\n",
    "    except ValueError:\n",
    "        pass  # Not a valid number, fall through to string comparison\n",
    "    \n",
    "    # Exact string comparison (handles non-numeric answers)\n",
    "    if answer.strip() == ground_truth.strip():\n",
    "        return 1.0\n",
    "    \n",
    "    return -0.5  # Answer found but wrong\n",
    "\n",
    "\n",
    "# Test cases from the book\n",
    "test_cases = [\n",
    "    (\"The answer is 42\",                \"42\",  1.0,   \"Correct - keyword extraction\"),\n",
    "    (\"Let me think... 6 * 7 = 42\",     \"42\",  1.0,   \"Correct - equals extraction\"),\n",
    "    (\"I think it's 43\",                 \"42\", -0.5,   \"Wrong - close but no cigar\"),\n",
    "    (\"I don't know\",                    \"42\", -1.0,   \"No answer - unparseable\"),\n",
    "    (\"result: 3.14159\",                 \"3.14159\", 1.0, \"Correct - float comparison\"),\n",
    "    (\"The answer is 3.141590001\",       \"3.14159\", 1.0, \"Correct - within tolerance\"),\n",
    "]\n",
    "\n",
    "print(\"Testing math_reward():\")\n",
    "print(f\"{'Response':<35} {'Truth':<10} {'Expected':<10} {'Got':<10} {'Note'}\")\n",
    "print(\"-\" * 85)\n",
    "for response, truth, expected, note in test_cases:\n",
    "    result = math_reward(response, truth)\n",
    "    status = \"pass\" if result == expected else \"FAIL\"\n",
    "    print(f\"{response:<35} {truth:<10} {expected:<10} {result:<10} {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Float Tolerance\n",
    "\n",
    "```python\n",
    "abs(float(answer) - float(ground_truth)) < 1e-6\n",
    "```\n",
    "\n",
    "This handles floating-point precision issues. Without it:\n",
    "- \"3.14159\" vs \"3.141590\" would fail string comparison\n",
    "- Numerical computation rounding errors would cause false negatives\n",
    "\n",
    "The tolerance of `1e-6` (0.000001) is tight enough to catch wrong answers but loose enough to forgive minor floating-point differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Code Reward - `code_reward()`\n",
    "\n",
    "For code generation tasks, we can use an even more objective measure: **does the code run and produce the right output?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def code_reward(code: str, expected_output: str, timeout: float = 5.0) -> float:\n",
    "    \"\"\"Execute generated code and check its output.\n",
    "    \n",
    "    Returns:\n",
    "        +1.0  if code runs and output matches expected\n",
    "        -0.5  if code runs but output is wrong\n",
    "        -1.0  if code crashes, times out, or fails to execute\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run the code in a subprocess with a timeout\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"-c\", code],  # -c means \"run this string as code\"\n",
    "            capture_output=True,       # Capture stdout and stderr\n",
    "            text=True,                 # Return strings, not bytes\n",
    "            timeout=timeout,           # Kill if it takes too long\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return -1.0  # Code crashed (syntax error, runtime error, etc.)\n",
    "        \n",
    "        if result.stdout.strip() == expected_output.strip():\n",
    "            return 1.0   # Correct output!\n",
    "        \n",
    "        return -0.5  # Code ran but gave wrong output\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return -1.0  # Infinite loop or too slow\n",
    "    except Exception:\n",
    "        return -1.0  # Any other failure\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"print(6 * 7)\",           \"42\",   1.0,  \"Correct code\"),\n",
    "    (\"print(6 * 8)\",           \"42\",  -0.5,  \"Wrong output (48 != 42)\"),\n",
    "    (\"syntax error here\",      \"42\",  -1.0,  \"Syntax error\"),\n",
    "    (\"while True: pass\",       \"42\",  -1.0,  \"Infinite loop (timeout)\"),\n",
    "    (\"print('hello world')\",   \"hello world\", 1.0, \"String output match\"),\n",
    "]\n",
    "\n",
    "print(\"Testing code_reward():\")\n",
    "print(f\"{'Code':<30} {'Expected':<15} {'Score':<10} {'Note'}\")\n",
    "print(\"-\" * 75)\n",
    "for code, expected, expected_score, note in test_cases:\n",
    "    # Skip the infinite loop test in notebook (would actually hang for 5 seconds)\n",
    "    if \"while True\" in code:\n",
    "        score = -1.0\n",
    "        print(f\"{code:<30} {expected:<15} {score:<10} {note} (skipped in notebook)\")\n",
    "    else:\n",
    "        score = code_reward(code, expected)\n",
    "        print(f\"{code:<30} {expected:<15} {score:<10} {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Note\n",
    "\n",
    "Running arbitrary generated code in a subprocess is a **sandbox** approach. The book keeps it simple with `subprocess.run`, but in production you'd want:\n",
    "- Docker containers\n",
    "- Resource limits (memory, disk)\n",
    "- Network isolation\n",
    "- Restricted syscalls (seccomp)\n",
    "\n",
    "The `timeout=5.0` parameter is the minimal safety measure - it kills the process if the generated code runs an infinite loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Think Token Rewards - `reward_with_thinking()`\n",
    "\n",
    "This is where it gets interesting. Chapter 6 introduces **think tokens** - special `<think>...</think>` blocks where the model shows its reasoning before giving a final answer.\n",
    "\n",
    "### The Four Reward Tiers\n",
    "\n",
    "```\n",
    "+1.0   Correct + has <think> block    (best: right answer WITH reasoning)\n",
    "+0.5   Correct + no <think> block     (good but: right answer without showing work)\n",
    "-0.2   Wrong + has <think> block      (mild penalty: wrong but at least tried reasoning)\n",
    "-0.5   Wrong + no <think> block       (bad: wrong AND didn't even try to reason)\n",
    "```\n",
    "\n",
    "### Why reward thinking even when wrong?\n",
    "\n",
    "The penalty for \"wrong + thinking\" (-0.2) is much milder than \"wrong + no thinking\" (-0.5). This creates a **gradient toward reasoning** even before the model can solve problems correctly. The model learns:\n",
    "\n",
    "1. First: \"I should use `<think>` blocks\" (because -0.2 > -0.5)\n",
    "2. Then: \"I should reason correctly in my `<think>` blocks\" (to reach +1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def reward_with_thinking(\n",
    "    response: str,\n",
    "    ground_truth: str,\n",
    "    base_reward_fn: Callable[[str, str], float] = math_reward,\n",
    ") -> float:\n",
    "    \"\"\"Composite reward that incentivizes explicit reasoning traces.\n",
    "    \n",
    "    Checks two things:\n",
    "    1. Is the answer correct? (via base_reward_fn)\n",
    "    2. Did the model show its reasoning? (via <think> tags)\n",
    "    \n",
    "    Combines both signals into a single scalar reward.\n",
    "    \"\"\"\n",
    "    # Check for thinking block\n",
    "    has_think = \"<think>\" in response and \"</think>\" in response\n",
    "    \n",
    "    # Get base correctness score\n",
    "    base_reward = base_reward_fn(response, ground_truth)\n",
    "    \n",
    "    # Combine into four tiers\n",
    "    if base_reward > 0 and has_think:\n",
    "        return 1.0    # Correct + thinking = best\n",
    "    elif base_reward > 0:\n",
    "        return 0.5    # Correct without thinking = good but not great\n",
    "    elif has_think:\n",
    "        return -0.2   # Wrong but showed reasoning = mild penalty\n",
    "    return -0.5       # Wrong without reasoning = standard penalty\n",
    "\n",
    "\n",
    "# Test all four tiers\n",
    "test_cases = [\n",
    "    # Correct + thinking\n",
    "    (\"<think>15 * 23 = 15*20 + 15*3 = 300 + 45 = 345</think>\\nThe answer is 345\",\n",
    "     \"345\", 1.0, \"Correct + think\"),\n",
    "    \n",
    "    # Correct, no thinking\n",
    "    (\"The answer is 345\",\n",
    "     \"345\", 0.5, \"Correct, no think\"),\n",
    "    \n",
    "    # Wrong + thinking\n",
    "    (\"<think>15 * 23 = 15*20 + 15*3 = 300 + 35 = 335</think>\\nThe answer is 335\",\n",
    "     \"345\", -0.2, \"Wrong + think\"),\n",
    "    \n",
    "    # Wrong, no thinking  \n",
    "    (\"The answer is 335\",\n",
    "     \"345\", -0.5, \"Wrong, no think\"),\n",
    "]\n",
    "\n",
    "print(\"Testing reward_with_thinking():\")\n",
    "print(f\"{'Scenario':<25} {'Expected':<10} {'Got':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for response, truth, expected, label in test_cases:\n",
    "    result = reward_with_thinking(response, truth)\n",
    "    status = \"pass\" if result == expected else \"FAIL\"\n",
    "    print(f\"{label:<25} {expected:<10} {result:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: How Rewards Shape GRPO Learning\n",
    "\n",
    "Let's trace through a concrete example of how rewards interact with the GRPO algorithm (Chapter 5).\n",
    "\n",
    "### Scenario: Group of 4 responses to \"What is 15 * 23?\"\n",
    "\n",
    "Using `math_reward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating what GRPO does with rewards\n",
    "\n",
    "responses = [\n",
    "    \"Let me calculate: 15 * 23 = 345. The answer is 345\",\n",
    "    \"15 times 23... I think it's 355\",\n",
    "    \"The answer is 345.\",\n",
    "    \"Hmm, I'm not sure about this one\",\n",
    "]\n",
    "ground_truth = \"345\"\n",
    "\n",
    "# Step 1: Compute rewards\n",
    "rewards = [math_reward(r, ground_truth) for r in responses]\n",
    "\n",
    "# Step 2: Compute baseline (group mean)\n",
    "baseline = sum(rewards) / len(rewards)\n",
    "\n",
    "# Step 3: Compute advantages\n",
    "advantages = [r - baseline for r in rewards]\n",
    "\n",
    "print(\"GRPO Reward Computation Example\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPrompt: 'What is 15 * 23?'  |  Ground truth: {ground_truth}\")\n",
    "print()\n",
    "\n",
    "for i, (resp, reward, adv) in enumerate(zip(responses, rewards, advantages)):\n",
    "    direction = \"ENCOURAGE\" if adv > 0 else (\"DISCOURAGE\" if adv < 0 else \"NEUTRAL\")\n",
    "    print(f\"Response {i+1}: '{resp[:50]}...'\")\n",
    "    print(f\"  Reward: {reward:+.1f}  |  Advantage: {adv:+.4f}  |  Action: {direction}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Baseline (mean reward): {baseline:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Responses 1 & 3 got +1.0 (correct), which is above the baseline\")\n",
    "print(\"    --> GRPO will INCREASE their probability\")\n",
    "print(\"  - Response 2 got -0.5 (wrong answer), below baseline\")\n",
    "print(\"    --> GRPO will DECREASE its probability\")\n",
    "print(\"  - Response 4 got -1.0 (no answer), well below baseline\")\n",
    "print(\"    --> GRPO will STRONGLY DECREASE its probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comparing Reward Functions\n",
    "\n",
    "Let's see how the same responses get scored differently with `math_reward` vs `reward_with_thinking`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt, different reward functions\n",
    "\n",
    "responses = [\n",
    "    \"<think>15*23 = 15*20 + 15*3 = 300 + 45 = 345</think>\\nThe answer is 345\",\n",
    "    \"The answer is 345\",\n",
    "    \"<think>15*23 = 15*20 + 15*3 = 300 + 35 = 335</think>\\nThe answer is 335\",\n",
    "    \"335\",\n",
    "]\n",
    "ground_truth = \"345\"\n",
    "\n",
    "print(\"Comparing math_reward vs reward_with_thinking\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "labels = [\n",
    "    \"Correct + thinking\",\n",
    "    \"Correct, no thinking\",\n",
    "    \"Wrong + thinking\",\n",
    "    \"Wrong, no thinking\",\n",
    "]\n",
    "\n",
    "print(f\"{'Scenario':<25} {'math_reward':<15} {'reward_with_thinking':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for label, resp in zip(labels, responses):\n",
    "    mr = math_reward(resp, ground_truth)\n",
    "    rt = reward_with_thinking(resp, ground_truth)\n",
    "    print(f\"{label:<25} {mr:+.1f}{'':>8} {rt:+.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"Key differences:\")\n",
    "print(\"  math_reward: Only cares about the final answer\")\n",
    "print(\"  reward_with_thinking: Also rewards the PROCESS of reasoning\")\n",
    "print()\n",
    "print(\"  With math_reward: 'Correct + thinking' and 'Correct, no thinking'\")\n",
    "print(\"    both get +1.0 -- no incentive to show reasoning.\")\n",
    "print(\"  With reward_with_thinking: 'Correct + thinking' gets +1.0 but\")\n",
    "print(\"    'Correct, no thinking' only gets +0.5 -- model learns to reason.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Reward Design Principles\n",
    "\n",
    "The book emphasizes several principles for designing reward functions:\n",
    "\n",
    "### 1. Reward shaping is everything\n",
    "The model will optimize for **exactly what you reward**. If your reward function has a loophole, the model will find it.\n",
    "\n",
    "### 2. Three tiers is a minimum\n",
    "Binary (correct/wrong) rewards provide less gradient signal than multi-tier scoring. The three tiers (+1.0, -0.5, -1.0) give GRPO more information to work with.\n",
    "\n",
    "### 3. Penalize non-answers harshly\n",
    "The -1.0 for \"no answer found\" is intentionally the strongest penalty. Without it, the model might learn to produce vague, non-committal responses that avoid being scored as \"wrong\".\n",
    "\n",
    "### 4. Process rewards are powerful\n",
    "`reward_with_thinking` is a simple form of **process-based reward** - it rewards not just the outcome but the process. Research shows this leads to more robust and generalizable reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The `__main__` Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the built-in test suite from verifier.py\n",
    "\n",
    "test_cases = [\n",
    "    (\"The answer is 42\", \"42\", 1.0),\n",
    "    (\"Let me think... 6 * 7 = 42\", \"42\", 1.0),\n",
    "    (\"I think it's 43\", \"42\", -0.5),\n",
    "    (\"I don't know\", \"42\", -1.0),\n",
    "]\n",
    "\n",
    "print(\"Testing math_reward (from verifier.py __main__):\")\n",
    "all_pass = True\n",
    "for response, truth, expected in test_cases:\n",
    "    result = math_reward(response, truth)\n",
    "    status = \"pass\" if result == expected else \"FAIL\"\n",
    "    if result != expected:\n",
    "        all_pass = False\n",
    "    print(f\"  {status} '{response[:30]}...' -> {result} (expected {expected})\")\n",
    "\n",
    "print(f\"\\n{'All tests passed!' if all_pass else 'Some tests FAILED!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Better Answer Extraction\n",
    "The current `extract_answer()` fails on responses like \"The answer is forty-two\". Can you add a pattern or preprocessing step to handle written-out numbers?\n",
    "\n",
    "### Exercise 2: Partial Credit\n",
    "The current system gives -0.5 for any wrong answer. Design a reward function that gives **partial credit** based on how close the answer is (e.g., if the truth is 345 and the model says 344, give a milder penalty than if it says 1000).\n",
    "\n",
    "### Exercise 3: Multi-Step Reward\n",
    "Extend `reward_with_thinking()` to also check that the intermediate arithmetic inside the `<think>` block is correct. For example, if the think block says \"15*20 = 350\" (wrong intermediate step), penalize it even if the final answer happens to be right.\n",
    "\n",
    "### Exercise 4: Reward Hacking\n",
    "Consider a model that learns to always output \"The answer is 42\" regardless of the question. Why would the current reward function encourage this? How could you modify the reward to prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Verifiable rewards** are deterministic functions that check objective correctness. They're cheap, fast, and reliable.\n",
    "\n",
    "2. **Answer extraction** uses cascading regex patterns from most-specific to least-specific to robustly find numbers in free-form text.\n",
    "\n",
    "3. **Three-tier scoring** (+1.0, -0.5, -1.0) provides more gradient signal than binary rewards and penalizes non-answers most harshly.\n",
    "\n",
    "4. **Think token rewards** incentivize the model to show its reasoning, creating a path toward process-based rewards.\n",
    "\n",
    "5. **Reward design is critical** - the model will optimize for exactly what you measure, including any loopholes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Previous:** [Chapter 2 - Rollouts](../ch02_rollouts/learn_rollouts.ipynb)  \n",
    "**Next:** [Chapter 5 - GRPO](../ch05_grpo/learn_grpo.ipynb) - How do we use these rewards to actually update the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}