{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Rollouts - Generating Training Data\n",
    "\n",
    "**Goal:** Understand how we generate diverse model responses (rollouts) that serve as training data for RL, and how QLoRA makes this possible on a single 24GB GPU.\n",
    "\n",
    "This notebook breaks down every function in `sample.py` and connects it to the book's explanations.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisite Concepts (Chapter 1 Recap)\n",
    "\n",
    "Before diving into rollouts, let's recall the **paradigm shift** from SFT to RL:\n",
    "\n",
    "| | SFT | RL |\n",
    "|---|---|---|\n",
    "| **Training signal** | Cross-entropy loss against a single target | Scalar reward for any generated output |\n",
    "| **Data** | Static (prompt, ideal_response) pairs | On-the-fly generation (rollouts) |\n",
    "| **Exploration** | None - always push toward one answer | Essential - sample diverse responses |\n",
    "| **Best for** | Style adoption, factual recall | Complex reasoning, creativity, safety |\n",
    "\n",
    "The fundamental limitation of SFT: it assumes there is **one correct answer** for every prompt. RL removes this constraint by letting the model **explore** and learn from a reward signal.\n",
    "\n",
    "### The RL Training Loop (High Level)\n",
    "\n",
    "```\n",
    "1. Prompt   -->  Pick a question from the dataset\n",
    "2. Generate -->  Sample one or more responses (ROLLOUTS - this chapter!)\n",
    "3. Score    -->  Evaluate each response with a reward function (Chapter 3)\n",
    "4. Learn    -->  Update model parameters (Chapters 4-5)\n",
    "Repeat.\n",
    "```\n",
    "\n",
    "This chapter focuses on **Step 2**: how do we efficiently generate responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Memory Problem\n",
    "\n",
    "### Why can't we just load an 8B model and start generating?\n",
    "\n",
    "Let's do the math for **Qwen3-8B-Instruct** (7.8 billion parameters):\n",
    "\n",
    "| Component | Memory (FP16) | Memory (FP32) |\n",
    "|---|---|---|\n",
    "| Model weights | 8B x 2 bytes = **16 GB** | 8B x 4 bytes = 32 GB |\n",
    "| Gradients | **16 GB** | 32 GB |\n",
    "| Optimizer states (Adam) | **32 GB** (2x weights) | 64 GB |\n",
    "| Activations | Variable (~2-8 GB) | Variable |\n",
    "| **Total** | **~66 GB** | ~130 GB |\n",
    "\n",
    "An RTX 3090 has **24 GB** of VRAM. Even just loading the weights in FP16 takes 16 GB, leaving only 8 GB for everything else.\n",
    "\n",
    "**Solution: QLoRA** (Quantized Low-Rank Adaptation) - a combination of two techniques that dramatically reduces memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: 4-Bit Quantization\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Instead of storing each weight as a 16-bit float (2 bytes), store it as a **4-bit integer** (~0.5 bytes). This cuts model weight memory from 16 GB to ~5 GB.\n",
    "\n",
    "### NF4 (NormalFloat4) Quantization\n",
    "\n",
    "The key insight: neural network weights follow an approximately **normal distribution**. NF4 exploits this by spacing its 16 quantization levels (4 bits = 2^4 = 16 values) to optimally cover the normal distribution, placing more levels near zero where most weights live.\n",
    "\n",
    "```\n",
    "Regular 4-bit:   [-8, -7, -6, ..., 0, ..., 6, 7]  (uniform spacing)\n",
    "NF4:             [-1.0, -0.69, -0.52, ..., 0, ..., 0.52, 0.69, 1.0]  (normal-distribution-optimal)\n",
    "```\n",
    "\n",
    "### The critical rule: quantized weights are FROZEN\n",
    "\n",
    "We never update the 4-bit weights during training. They serve as a compressed representation of the pretrained model's knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see this in code\n",
    "\n",
    "Here's how `sample.py` configures 4-bit quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# This configuration tells HuggingFace Transformers to load the model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Use NormalFloat4 (optimal for neural net weights)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16 during forward pass\n",
    ")\n",
    "\n",
    "print(\"Quantization config created.\")\n",
    "print(f\"  load_in_4bit: {bnb_config.load_in_4bit}\")\n",
    "print(f\"  quant_type: {bnb_config.bnb_4bit_quant_type}\")\n",
    "print(f\"  compute_dtype: {bnb_config.bnb_4bit_compute_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key detail: `bnb_4bit_compute_dtype=torch.bfloat16`**\n",
    "\n",
    "While weights are *stored* in 4-bit, during the forward pass they are temporarily **dequantized** to bfloat16 for computation. This gives us the memory savings of 4-bit storage with the numerical precision of 16-bit computation.\n",
    "\n",
    "```\n",
    "Storage:      4-bit (NF4)    --> ~5 GB for 8B params\n",
    "Computation:  bfloat16       --> accurate matrix multiplications\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Since the base weights are frozen (4-bit, can't be updated), we need **something** trainable. LoRA injects small trainable matrices alongside the frozen weights.\n",
    "\n",
    "### How it works\n",
    "\n",
    "For a weight matrix **W** of size `(d x d)` in the transformer:\n",
    "\n",
    "```\n",
    "Original:  y = W @ x              (d x d matrix, millions of parameters)\n",
    "\n",
    "With LoRA: y = W @ x + B @ A @ x  (W frozen, A is d x r, B is r x d)\n",
    "```\n",
    "\n",
    "Where `r` (the rank) is much smaller than `d`. For Qwen3-8B with hidden_size=4096:\n",
    "\n",
    "```\n",
    "Original W:        4096 x 4096 = 16,777,216 parameters\n",
    "LoRA (r=64):  A =  4096 x 64   =    262,144 parameters\n",
    "              B =    64 x 4096  =    262,144 parameters\n",
    "              Total:                 524,288 parameters  (3.1% of original!)\n",
    "```\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "Research has shown that the weight updates during fine-tuning tend to be **low-rank** - they don't need the full dimensionality of the weight matrix. LoRA exploits this by directly parameterizing the update as a low-rank matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# This configures which layers get LoRA adapters and how large they are\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                # Rank of the low-rank matrices (higher = more expressive, more memory)\n",
    "    lora_alpha=16,       # Scaling factor: effective learning rate multiplier = alpha/r = 16/64 = 0.25\n",
    "    target_modules=[     # Which layers to add LoRA adapters to:\n",
    "        \"q_proj\",        #   Query projection in self-attention\n",
    "        \"v_proj\",        #   Value projection in self-attention\n",
    "        \"k_proj\",        #   Key projection in self-attention\n",
    "        \"o_proj\",        #   Output projection in self-attention\n",
    "    ],\n",
    "    lora_dropout=0.05,   # Dropout on LoRA activations for regularization\n",
    "    task_type=\"CAUSAL_LM\",  # We're doing causal language modeling\n",
    ")\n",
    "\n",
    "print(\"LoRA config created.\")\n",
    "print(f\"  Rank: {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Effective scaling: {lora_config.lora_alpha / lora_config.r}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `lora_alpha`\n",
    "\n",
    "The actual LoRA forward pass is:\n",
    "\n",
    "```\n",
    "y = W @ x + (alpha / r) * B @ A @ x\n",
    "```\n",
    "\n",
    "With `alpha=16, r=64`, the scaling factor is `16/64 = 0.25`. This means the LoRA adapter's contribution is scaled down by 4x. This prevents the randomly-initialized adapters from disrupting the pretrained model too much at the start of training.\n",
    "\n",
    "### Why target attention projections?\n",
    "\n",
    "The attention mechanism is where the model learns **what to attend to** when generating responses. By adapting Q, K, V, and O projections, we modify:\n",
    "- **Q (query)**: What the model is looking for\n",
    "- **K (key)**: What information is available\n",
    "- **V (value)**: What information is retrieved\n",
    "- **O (output)**: How retrieved information is combined\n",
    "\n",
    "This gives us maximum leverage for changing the model's behavior with minimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Putting It Together - `load_model_qlora()`\n",
    "\n",
    "Now let's trace through the complete model loading function from `sample.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full walkthrough of load_model_qlora() ===\n",
    "# (Run this cell only if you have a GPU with enough VRAM)\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def load_model_qlora(model_name: str = \"Qwen/Qwen3-8B-Instruct\"):\n",
    "    # STEP 1: Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # STEP 2: Load the pretrained model with quantization\n",
    "    # device_map=\"auto\" places layers across available GPUs automatically\n",
    "    # trust_remote_code=True allows running Qwen's custom modeling code\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # STEP 3: Load the tokenizer (handles text <-> token conversion)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # STEP 4: Configure and attach LoRA adapters\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # get_peft_model wraps the original model, injecting LoRA layers\n",
    "    # After this call:\n",
    "    #   - Base weights: frozen, 4-bit (~5 GB)\n",
    "    #   - LoRA adapters: trainable, float32 (~200 MB)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # This prints something like:\n",
    "    # \"trainable params: 50,331,648 || all params: 7,865,956,352 || trainable%: 0.6398%\"\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Uncomment to run (requires GPU):\n",
    "# model, tokenizer = load_model_qlora()\n",
    "print(\"Function defined. Uncomment the last line to load the model (requires GPU).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory breakdown after loading\n",
    "\n",
    "```\n",
    "Base model (4-bit NF4):     ~5 GB   (frozen, not trainable)\n",
    "LoRA adapters (float32):    ~0.2 GB (trainable, ~50M params)\n",
    "CUDA overhead:              ~0.5 GB\n",
    "Total:                      ~5.7 GB\n",
    "\n",
    "Remaining for training:     ~18.3 GB (on a 24GB GPU)\n",
    "```\n",
    "\n",
    "Compare this to full fine-tuning (~66 GB) or even regular LoRA without quantization (~18 GB just for weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Sampling Responses - Temperature & Top-p\n",
    "\n",
    "### Why sampling matters for RL\n",
    "\n",
    "In SFT, we typically use **greedy decoding** (always pick the most likely token). In RL, we need **diverse responses** to explore the space of possible answers. This is where temperature and top-p sampling come in.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "Temperature scales the logits before applying softmax:\n",
    "\n",
    "```\n",
    "P(token_i) = exp(logit_i / T) / sum_j(exp(logit_j / T))\n",
    "```\n",
    "\n",
    "| Temperature | Effect | Use case |\n",
    "|---|---|---|\n",
    "| T = 0.1 | Nearly deterministic, always picks top token | Evaluation |\n",
    "| T = 0.7 | Balanced diversity | **RL training (our choice)** |\n",
    "| T = 1.0 | Standard sampling | Creative writing |\n",
    "| T > 1.0 | Very random, flattened distribution | Brainstorming |\n",
    "\n",
    "### Top-p (Nucleus Sampling)\n",
    "\n",
    "After applying temperature, sort tokens by probability. Include only the smallest set of tokens whose cumulative probability exceeds `p`:\n",
    "\n",
    "```\n",
    "Example with top_p=0.9:\n",
    "  Token A: 0.50  --> cumulative: 0.50 (included)\n",
    "  Token B: 0.25  --> cumulative: 0.75 (included)\n",
    "  Token C: 0.10  --> cumulative: 0.85 (included)\n",
    "  Token D: 0.07  --> cumulative: 0.92 (included, crosses 0.9)\n",
    "  Token E: 0.05  --> EXCLUDED\n",
    "  Token F: 0.03  --> EXCLUDED\n",
    "```\n",
    "\n",
    "This prevents the model from occasionally selecting bizarre, low-probability tokens while still allowing diversity among reasonable choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizing temperature's effect on sampling ===\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulate logits for 5 tokens (as if the model predicted these)\n",
    "logits = torch.tensor([2.0, 1.5, 0.5, -0.5, -1.0])\n",
    "token_names = [\"345\", \"350\", \"342\", \"1000\", \"banana\"]\n",
    "\n",
    "print(\"Token probabilities at different temperatures:\")\n",
    "print(f\"{'Token':<10} {'T=0.1':<10} {'T=0.7':<10} {'T=1.0':<10} {'T=2.0':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for temp in [0.1, 0.7, 1.0, 2.0]:\n",
    "    probs = F.softmax(logits / temp, dim=0)\n",
    "    if temp == 0.1:\n",
    "        for i, name in enumerate(token_names):\n",
    "            print(f\"{name:<10}\", end=\"\")\n",
    "            for t in [0.1, 0.7, 1.0, 2.0]:\n",
    "                p = F.softmax(logits / t, dim=0)[i].item()\n",
    "                print(f\"{p:<10.4f}\", end=\"\")\n",
    "            print()\n",
    "        break\n",
    "\n",
    "print(\"\\nNotice how T=0.1 puts 99%+ on '345', while T=2.0 spreads probability more evenly.\")\n",
    "print(\"T=0.7 (our choice) keeps '345' most likely but gives '350' and '342' reasonable chances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The `sample_response()` Function - Line by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 256,\n",
    ") -> str:\n",
    "    # STEP 1: Format the prompt using the model's chat template\n",
    "    # Qwen3 expects a specific format:\n",
    "    #   <|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,           # Return string, not token IDs\n",
    "        add_generation_prompt=True  # Add the \"assistant\" prefix so model starts responding\n",
    "    )\n",
    "\n",
    "    # STEP 2: Tokenize and move to GPU\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # STEP 3: Generate response tokens\n",
    "    # torch.no_grad() because rollouts are INFERENCE ONLY\n",
    "    # We don't need gradients during generation - only during the GRPO loss computation later\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,  # Cap response length\n",
    "            temperature=temperature,         # 0.7 for balanced diversity\n",
    "            do_sample=True,                  # Enable sampling (vs greedy)\n",
    "            top_p=0.9,                       # Nucleus sampling\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Avoid padding warnings\n",
    "        )\n",
    "\n",
    "    # STEP 4: Decode only the NEW tokens (not the prompt)\n",
    "    # outputs[0] contains [prompt_tokens..., response_tokens...]\n",
    "    # inputs.input_ids.shape[1] gives us the length of the prompt\n",
    "    # So outputs[0][prompt_length:] gives us just the response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"sample_response() defined. See annotations above for line-by-line explanation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output slicing trick explained\n",
    "\n",
    "```python\n",
    "outputs[0][inputs.input_ids.shape[1]:]\n",
    "```\n",
    "\n",
    "This is important to understand. `model.generate()` returns the **full sequence** including the prompt:\n",
    "\n",
    "```\n",
    "outputs[0] = [<prompt tokens>, <generated tokens>]\n",
    "              |--- shape[1] ---|--- new tokens ---|\n",
    "```\n",
    "\n",
    "By slicing from `shape[1]` onward, we extract only what the model generated.\n",
    "\n",
    "### Why `torch.no_grad()`?\n",
    "\n",
    "During rollout generation, we're just collecting samples - we don't need gradients. The gradient computation happens later in the GRPO step (Chapter 5) when we compute log-probabilities of the generated responses. This saves significant memory during generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Batch Sampling - `sample_batch()`\n",
    "\n",
    "For GRPO (Chapter 5), we need **multiple responses per prompt** to compute group-relative advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    n: int = 4,               # Number of responses to generate (G in GRPO)\n",
    "    temperature: float = 0.7,\n",
    ") -> list[str]:\n",
    "    # Generate n independent responses for the same prompt\n",
    "    # Each call to sample_response uses stochastic sampling,\n",
    "    # so each response will be different (due to temperature > 0)\n",
    "    return [sample_response(model, tokenizer, prompt, temperature) for _ in range(n)]\n",
    "\n",
    "\n",
    "# Example usage (without running):\n",
    "# responses = sample_batch(model, tokenizer, \"What is 15 * 23?\", n=4)\n",
    "# This would generate 4 different answers to the same question.\n",
    "# Some might be correct (345), others might be wrong.\n",
    "# GRPO uses this diversity to learn which responses are better.\n",
    "\n",
    "print(\"sample_batch() generates n diverse responses for the same prompt.\")\n",
    "print(\"\")\n",
    "print(\"Example: 4 responses to 'What is 15 * 23?':\")\n",
    "print(\"  Response 1: 'Let me calculate... 15 * 23 = 345'         --> reward: +1.0\")\n",
    "print(\"  Response 2: '15 times 23 is 355'                         --> reward: -0.5\")\n",
    "print(\"  Response 3: 'The answer is 345.'                         --> reward: +1.0\")\n",
    "print(\"  Response 4: 'Hmm, I think it might be around 300'        --> reward: -0.5\")\n",
    "print(\"\")\n",
    "print(\"GRPO will increase the probability of responses 1 and 3,\")\n",
    "print(\"and decrease the probability of responses 2 and 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Chat Templates - Why They Matter\n",
    "\n",
    "The `apply_chat_template` call is easy to overlook but critical. Different models expect different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating what a chat template does (conceptual - no model needed)\n",
    "\n",
    "# What the user types:\n",
    "user_prompt = \"What is 15 * 23?\"\n",
    "\n",
    "# What the model actually sees after apply_chat_template (Qwen3 format):\n",
    "formatted = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"You are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n\"\n",
    "    f\"{user_prompt}<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant\\n\"  # <-- This is added by add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Raw user prompt:\")\n",
    "print(f\"  '{user_prompt}'\")\n",
    "print()\n",
    "print(\"After apply_chat_template:\")\n",
    "print(f\"  '{formatted}'\")\n",
    "print()\n",
    "print(\"The model generates tokens AFTER the 'assistant\\\\n' marker.\")\n",
    "print(\"Without proper formatting, the model would produce garbage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: The `__main__` Block - Running `sample.py` Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what happens when you run: python ch02_rollouts/sample.py\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Loading model with QLoRA...\")\n",
    "#     model, tokenizer = load_model_qlora()      # Load Qwen3-8B in 4-bit + LoRA\n",
    "#\n",
    "#     prompt = \"What is 15 * 23?\"\n",
    "#     print(f\"Prompt: {prompt}\")\n",
    "#\n",
    "#     responses = sample_batch(model, tokenizer, prompt, n=4)  # Generate 4 responses\n",
    "#     for i, r in enumerate(responses):\n",
    "#         print(f\"Response {i+1}: {r[:200]}...\")   # Print first 200 chars of each\n",
    "\n",
    "print(\"The standalone script:\")\n",
    "print(\"  1. Loads Qwen3-8B-Instruct with QLoRA (~5.7 GB VRAM)\")\n",
    "print(\"  2. Generates 4 diverse responses to '15 * 23'\")\n",
    "print(\"  3. Prints the first 200 characters of each\")\n",
    "print()\n",
    "print(\"This is the foundation that Chapter 5 (GRPO) builds on.\")\n",
    "print(\"GRPO will call sample_batch() to generate groups of responses,\")\n",
    "print(\"then use reward signals to determine which are better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Temperature Exploration\n",
    "Try generating responses with different temperatures (0.1, 0.3, 0.7, 1.0, 1.5) for the same math problem. How does diversity change? At what temperature do you start getting incorrect answers?\n",
    "\n",
    "### Exercise 2: LoRA Rank Experiment\n",
    "Change `r=64` to `r=8` and `r=128`. How does this affect:\n",
    "- Number of trainable parameters?\n",
    "- Memory usage?\n",
    "- Quality of generations after a few training steps?\n",
    "\n",
    "### Exercise 3: Target Modules\n",
    "The code applies LoRA to Q, K, V, O projections. Try adding `\"gate_proj\"`, `\"up_proj\"`, `\"down_proj\"` (the MLP layers). Does this improve training at the cost of more memory?\n",
    "\n",
    "### Exercise 4: Understand the Slicing\n",
    "If the prompt tokenizes to 25 tokens and the model generates 50 new tokens, what is `outputs[0].shape`? What does `outputs[0][25:]` contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **QLoRA = Quantization + LoRA**: Store base model in 4-bit (~5 GB), train tiny adapters in full precision (~0.2 GB). This is what makes single-GPU RL possible.\n",
    "\n",
    "2. **Rollouts are inference-only**: We generate responses with `torch.no_grad()`. Gradients come later during the GRPO loss computation.\n",
    "\n",
    "3. **Temperature controls exploration**: T=0.7 is the sweet spot for RL - enough diversity to discover good strategies, not so much that responses are random.\n",
    "\n",
    "4. **Top-p prevents catastrophic samples**: Even with temperature, nucleus sampling ensures we never pick truly bizarre tokens.\n",
    "\n",
    "5. **Chat templates are essential**: The model was trained on a specific format. Using `apply_chat_template()` ensures we match it.\n",
    "\n",
    "6. **Group sampling enables GRPO**: By generating multiple responses per prompt, we can compute relative advantages without a critic network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next:** [Chapter 3 - Reward Signals](../ch03_rewards/learn_rewards.ipynb) - How do we score the responses we just generated?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}